# .github/scripts/fetch_tech_news.py
import feedparser
import time
from datetime import datetime, timedelta, timezone
import requests
import os

# é…ç½®
FEED_URLS = [
    "https://www.infoq.cn/aibriefs"
]

SERVERCHAN_SENDKEY = os.getenv("SERVERCHAN_SENDKEY")
if not SERVERCHAN_SENDKEY:
    raise EnvironmentError("SERVERCHAN_SENDKEY environment variable is not set.")

# è®¡ç®— 24 å°æ—¶å‰çš„æ—¶é—´ï¼ˆUTCï¼‰
now = datetime.now(timezone.utc)
one_day_ago = now - timedelta(hours=24)

def is_recent(entry):
    """åˆ¤æ–­æ–‡ç« æ˜¯å¦åœ¨è¿‡å»24å°æ—¶å†…å‘å¸ƒ"""
    pub_time = None
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        pub_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        pub_time = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
    return pub_time and pub_time >= one_day_ago

def fetch_news():
    articles = []
    for url in FEED_URLS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                if is_recent(entry):
                    title = entry.get('title', 'No Title')
                    link = entry.get('link', '#')
                    articles.append(f"<a href='{link}'>{title}</a>")
        except Exception as e:
            print(f"Failed to fetch {url}: {e}")
    return articles

def fetch_latest_articles():
    url = "https://www.infoq.cn/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        articles = []
        # æ ¹æ®å½“å‰ç½‘é¡µç»“æ„ï¼Œæå–æ–‡ç« é“¾æ¥å’Œæ ‡é¢˜ï¼ˆéœ€å®šæœŸç»´æŠ¤é€‰æ‹©å™¨ï¼‰
        for item in soup.select('a[href^="/article/"], a[href^="/news/"]'):
            title_elem = item.find(['h2', 'h3', 'div'], class_=lambda x: x and 'title' in x.lower())
            desc_elem = item.find(['p', 'div'], class_=lambda x: x and ('desc' in x or 'summary' in x))
            
            title = title_elem.get_text(strip=True) if title_elem else item.get_text(strip=True)
            desc = desc_elem.get_text(strip=True) if desc_elem else ''
            link = "https://www.infoq.cn" + item['href'] if item['href'].startswith('/') else item['href']

            if title and link not in [a['link'] for a in articles]:
                articles.append({
                    "title": title,
                    "description": desc,
                    "link": link,
                    "time_fetched": datetime.now().strftime("%Y-%m-%d %H:%M")
                })
                if len(articles) >= 10:  # åªå–å‰10ç¯‡
                    break

        return articles
    except Exception as e:
        print(f"[Error] {e}")
        return []

def send_to_serverchan(title, content):
    url = f"https://sctapi.ftqq.com/{SERVERCHAN_SENDKEY}.send"
    data = {
        "title": title,
        "desp": content,
        "channel": "9"  # é»˜è®¤æ¨é€è‡³ä¼ä¸šå¾®ä¿¡ï¼ˆå…¼å®¹æ€§æœ€å¥½ï¼‰
    }
    response = requests.post(url, data=data)
    if response.status_code == 200:
        print("âœ… æ¶ˆæ¯æ¨é€æˆåŠŸï¼")
    else:
        print(f"âŒ æ¨é€å¤±è´¥: {response.text}")

if __name__ == "__main__":
    print("ğŸ” æ­£åœ¨æŠ“å–è¿‡å»24å°æ—¶çš„ç§‘æŠ€æ–°é—»...")
    news = fetch_latest_articles()

    if not news:
        print("ğŸ“­ æœªæ‰¾åˆ°è¿‡å»24å°æ—¶å†…çš„æ–°ç§‘æŠ€æ–°é—»ã€‚")
        send_to_serverchan("ç§‘æŠ€æ—¥æŠ¥", "ğŸ“­ ä»Šæ—¥æš‚æ— æ–°ç§‘æŠ€æ–°é—»ã€‚")
    else:
        content = "<br>".join(news[:20])  # æœ€å¤šæ¨é€20æ¡
        send_to_serverchan("ğŸ“° è¿‡å»24å°æ—¶ç§‘æŠ€è¦é—»", content)
